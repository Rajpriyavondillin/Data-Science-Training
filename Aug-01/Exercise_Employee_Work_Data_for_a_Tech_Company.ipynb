{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario: Employee Work Data for a Tech Company"
      ],
      "metadata": {
        "id": "lTJOZffJCySP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Prepare Data in PySpark"
      ],
      "metadata": {
        "id": "2bsQ6Z8SC0Pj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnLA_Ro_-vn4",
        "outputId": "9be13219-4cb2-4657-9799-d95680b6b512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID|Name |Department |Project        |Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|101  |Ravi |Engineering|AI Engine      |95000 |42          |\n",
            "|102  |Sneha|Engineering|Data Platform  |87000 |45          |\n",
            "|103  |Kabir|Marketing  |Product Launch |65000 |40          |\n",
            "|104  |Anita|Sales      |Client Outreach|70000 |38          |\n",
            "|105  |Divya|Engineering|AI Engine      |99000 |48          |\n",
            "|106  |Amit |Marketing  |Social Media   |62000 |35          |\n",
            "|107  |Priya|HR         |Policy Revamp  |58000 |37          |\n",
            "|108  |Manav|Sales      |Lead Gen       |73000 |41          |\n",
            "|109  |Neha |Engineering|Security Suite |91000 |46          |\n",
            "|110  |Farah|HR         |Onboarding     |60000 |36          |\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EmployeeWorkData\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = [\n",
        "    Row(EmpID=101, Name=\"Ravi\", Department=\"Engineering\", Project=\"AI Engine\",Salary=95000, HoursPerWeek=42),\n",
        "    Row(EmpID=102, Name=\"Sneha\", Department=\"Engineering\", Project=\"Data Platform\",Salary=87000, HoursPerWeek=45),\n",
        "    Row(EmpID=103, Name=\"Kabir\", Department=\"Marketing\", Project=\"Product Launch\",Salary=65000, HoursPerWeek=40),\n",
        "    Row(EmpID=104, Name=\"Anita\", Department=\"Sales\", Project=\"Client Outreach\",Salary=70000, HoursPerWeek=38),\n",
        "    Row(EmpID=105, Name=\"Divya\", Department=\"Engineering\", Project=\"AI Engine\",Salary=99000, HoursPerWeek=48),\n",
        "    Row(EmpID=106, Name=\"Amit\", Department=\"Marketing\", Project=\"Social Media\",Salary=62000, HoursPerWeek=35),\n",
        "    Row(EmpID=107, Name=\"Priya\", Department=\"HR\", Project=\"Policy Revamp\",Salary=58000, HoursPerWeek=37),\n",
        "    Row(EmpID=108, Name=\"Manav\", Department=\"Sales\", Project=\"Lead Gen\", Salary=73000,HoursPerWeek=41),\n",
        "    Row(EmpID=109, Name=\"Neha\", Department=\"Engineering\", Project=\"Security Suite\",Salary=91000, HoursPerWeek=46),\n",
        "    Row(EmpID=110, Name=\"Farah\", Department=\"HR\", Project=\"Onboarding\", Salary=60000,HoursPerWeek=36)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Create Views"
      ],
      "metadata": {
        "id": "U9ZWNzK4DJXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Local Temp View\n",
        "df.createOrReplaceTempView(\"employees_local\")\n",
        "\n",
        "# Create a Global Temp View\n",
        "df.createOrReplaceGlobalTempView(\"employees_global\")"
      ],
      "metadata": {
        "id": "apSk0UA7DM1o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A: Exercises on Local View ( employees_local )\n",
        "\n",
        "1. List all employees working on the \"AI Engine\" project.\n",
        "2. Show all employees from the \"Marketing\" department with salaries greater than\n",
        "60,000.\n",
        "3. Calculate the average salary for each department.\n",
        "4. List the top 3 highest paid employees overall.\n",
        "5. Find employees who work more than 40 hours per week.\n",
        "6. Group by project and display the number of employees per project.\n",
        "7. Drop the local view. Try querying again â€” what happens?"
      ],
      "metadata": {
        "id": "4G-JcHfKDRjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. List all employees working on the \"AI Engine\" project.\n",
        "spark.sql(\"SELECT * FROM employees_local WHERE Project = 'AI Engine'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl47A2yBDTQC",
        "outputId": "56e7867f-2631-40ad-f177-af0c0cf0ebdb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------+------+------------+\n",
            "|EmpID| Name| Department|  Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "|  101| Ravi|Engineering|AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|AI Engine| 99000|          48|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Show all employees from the \"Marketing\" department with salaries greater than 60,000.\n",
        "spark.sql(\"SELECT * FROM employees_local WHERE Department = 'Marketing' AND Salary > 60000\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADhczzI6EQeZ",
        "outputId": "36457d0c-3ec7-4d27-80e1-bcdc0af23219"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+--------------+------+------------+\n",
            "|EmpID| Name|Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|  103|Kabir| Marketing|Product Launch| 65000|          40|\n",
            "|  106| Amit| Marketing|  Social Media| 62000|          35|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Calculate the average salary for each department.\n",
        "spark.sql(\"SELECT Department, AVG(Salary) AS AvgSalary FROM employees_local GROUP BY Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls3MmINEEZvD",
        "outputId": "6af2075f-3c30-4ca7-d717-64c158cef68b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+\n",
            "| Department|AvgSalary|\n",
            "+-----------+---------+\n",
            "|      Sales|  71500.0|\n",
            "|Engineering|  93000.0|\n",
            "|  Marketing|  63500.0|\n",
            "|         HR|  59000.0|\n",
            "+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. List the top 3 highest paid employees overall.\n",
        "spark.sql(\"SELECT Name, Salary FROM employees_local ORDER BY Salary DESC LIMIT 3\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOuLr0adEfv5",
        "outputId": "fcbd69c2-2dc0-469f-ac20-a3bcc9de3a30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Name|Salary|\n",
            "+-----+------+\n",
            "|Divya| 99000|\n",
            "| Ravi| 95000|\n",
            "| Neha| 91000|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Find employees who work more than 40 hours per week.\n",
        "spark.sql(\"SELECT * FROM employees_local WHERE HoursPerWeek > 40\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGvmQqDrEqXo",
        "outputId": "bdabaadc-dcda-4096-b74e-6ba529f3452c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Group by project and display the number of employees per project.\n",
        "spark.sql(\"SELECT Project, COUNT(*) AS EmployeeCount FROM employees_local GROUP BY Project\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcW615uxExjy",
        "outputId": "2be5394a-32f7-460d-dec8-58ae13642ad4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+\n",
            "|        Project|EmployeeCount|\n",
            "+---------------+-------------+\n",
            "|  Data Platform|            1|\n",
            "|      AI Engine|            2|\n",
            "| Product Launch|            1|\n",
            "|Client Outreach|            1|\n",
            "| Security Suite|            1|\n",
            "|  Policy Revamp|            1|\n",
            "|       Lead Gen|            1|\n",
            "|   Social Media|            1|\n",
            "|     Onboarding|            1|\n",
            "+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Drop the local view. Try querying again â€” what happens?\n",
        "\n",
        "# Drop view\n",
        "spark.catalog.dropTempView(\"employees_local\")\n",
        "\n",
        "# Try querying again (this will throw an error)\n",
        "spark.sql(\"SELECT * FROM employees_local\").show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "du9JS3ySE5Hb",
        "outputId": "2c21d4fb-c263-4ab7-9568-3d9732d43152"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees_local], [], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1676893846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Try querying again (this will throw an error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM employees_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees_local], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B: Exercises on Global View ( employees_global )"
      ],
      "metadata": {
        "id": "BiR_96BvFIBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Retrieve all \"HR\" employees working fewer than 38 hours/week.\n",
        "2. Calculate the total salary payout for each department.\n",
        "3. For each employee, add a derived column Status :\n",
        "\n",
        "    If HoursPerWeek > 45 â†’ 'Overworked'\n",
        "\n",
        "    Otherwise â†’ 'Normal'\n",
        "4. Count the total number of employees working on each \"Project\" .\n",
        "5. List employees whose salary is above the average salary in their department.\n",
        "6. Open a new Spark session and query \"global_temp.employees_global\" from there."
      ],
      "metadata": {
        "id": "3GBkgxi0FNVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Retrieve all \"HR\" employees working fewer than 38 hours/week.\n",
        "spark.sql(\"SELECT * FROM global_temp.employees_global WHERE Department = 'HR' AND HoursPerWeek < 38\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91eCrTLjFG7A",
        "outputId": "e3f7e537-4ec8-4382-e8df-a17d2287a409"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+-------------+------+------------+\n",
            "|EmpID| Name|Department|      Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "|  107|Priya|        HR|Policy Revamp| 58000|          37|\n",
            "|  110|Farah|        HR|   Onboarding| 60000|          36|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Calculate the total salary payout for each department.\n",
        "spark.sql(\"SELECT Department, SUM(Salary) AS TotalSalary FROM global_temp.employees_global GROUP BY Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxICL448Fnp6",
        "outputId": "66313c0b-351c-44ce-938b-39c4c66c9b5e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "| Department|TotalSalary|\n",
            "+-----------+-----------+\n",
            "|      Sales|     143000|\n",
            "|Engineering|     372000|\n",
            "|  Marketing|     127000|\n",
            "|         HR|     118000|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. For each employee, add a derived column Status :\n",
        "    # If HoursPerWeek > 45 â†’ 'Overworked'\n",
        "    # Otherwise â†’ 'Normal'\n",
        "spark.sql(\"\"\"\n",
        "    SELECT *,\n",
        "        CASE\n",
        "            WHEN HoursPerWeek > 45 THEN 'Overworked'\n",
        "            ELSE 'Normal'\n",
        "        END AS Status\n",
        "    FROM global_temp.employees_global\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV35bOSjFq02",
        "outputId": "5d7f1768-af09-4fe0-c781-39dcfc7edfb2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|    Status|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|    Normal|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|    Normal|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|    Normal|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|    Normal|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|Overworked|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|    Normal|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|    Normal|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|    Normal|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|Overworked|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|    Normal|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Count the total number of employees working on each \"Project\" .\n",
        "spark.sql(\"SELECT Project, COUNT(*) AS EmployeeCount FROM global_temp.employees_global GROUP BY Project\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYK1g-oPF01t",
        "outputId": "53277c01-6d7d-446f-d5d4-3a8ac5d3b940"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+\n",
            "|        Project|EmployeeCount|\n",
            "+---------------+-------------+\n",
            "|  Data Platform|            1|\n",
            "|      AI Engine|            2|\n",
            "| Product Launch|            1|\n",
            "|Client Outreach|            1|\n",
            "| Security Suite|            1|\n",
            "|  Policy Revamp|            1|\n",
            "|       Lead Gen|            1|\n",
            "|   Social Media|            1|\n",
            "|     Onboarding|            1|\n",
            "+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. List employees whose salary is above the average salary in their department.\n",
        "spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM global_temp.employees_global g1\n",
        "    WHERE Salary > (\n",
        "        SELECT AVG(Salary)\n",
        "        FROM global_temp.employees_global g2\n",
        "        WHERE g2.Department = g1.Department\n",
        "        )\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0eMEil_F4VS",
        "outputId": "a01d90a4-b7c7-4f6f-e38c-63c60007dd92"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  103|Kabir|  Marketing|Product Launch| 65000|          40|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  110|Farah|         HR|    Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Open a new Spark session and query \"global_temp.employees_global\" from there.\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# New session\n",
        "new_spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
        "\n",
        "# Query global view from new session\n",
        "new_spark.sql(\"SELECT * FROM global_temp.employees_global\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHd32D9eGCDS",
        "outputId": "b75e831a-174d-4e9b-af2b-8ceee5c64392"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Challenges"
      ],
      "metadata": {
        "id": "Mzlcv84LGTuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Use a window function to assign rank to employees within each department based\n",
        "on salary.\n",
        "2. Create another view (local or global) that only contains \"Engineering\"\n",
        "employees.\n",
        "3. Create a SQL view that filters out all employees working < 38 hours and saves\n",
        "it as \"active_employees\" ."
      ],
      "metadata": {
        "id": "EZMdE8h6GVju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Use a window function to assign rank to employees within each department based on salary.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "windowSpec = Window.partitionBy(\"Department\").orderBy(df[\"Salary\"].desc())\n",
        "df.withColumn(\"Rank\", rank().over(windowSpec)).select(\"Name\", \"Department\", \"Salary\", \"Rank\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJSqyi0BGYeO",
        "outputId": "381d13c0-b208-4d71-f239-726ac13ff07e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+------+----+\n",
            "| Name| Department|Salary|Rank|\n",
            "+-----+-----------+------+----+\n",
            "|Divya|Engineering| 99000|   1|\n",
            "| Ravi|Engineering| 95000|   2|\n",
            "| Neha|Engineering| 91000|   3|\n",
            "|Sneha|Engineering| 87000|   4|\n",
            "|Farah|         HR| 60000|   1|\n",
            "|Priya|         HR| 58000|   2|\n",
            "|Kabir|  Marketing| 65000|   1|\n",
            "| Amit|  Marketing| 62000|   2|\n",
            "|Manav|      Sales| 73000|   1|\n",
            "|Anita|      Sales| 70000|   2|\n",
            "+-----+-----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create another view (local or global) that only contains \"Engineering\" employees.\n",
        "\n",
        "# Filter employees from Engineering department\n",
        "df_engineering = df.filter(df.Department == \"Engineering\")\n",
        "\n",
        "# Create a local temporary view named \"engineering_employees\"\n",
        "df_engineering.createOrReplaceTempView(\"engineering_employees\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM engineering_employees\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvshUZnmGeh2",
        "outputId": "9c69e00c-c7e6-4ba0-fd7c-fa76c5da04dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a SQL view that filters out all employees working < 38 hours and saves it as \"active_employees\" .\n",
        "df.createOrReplaceTempView(\"all_employees\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMP VIEW active_employees AS\n",
        "    SELECT *\n",
        "    FROM all_employees\n",
        "    WHERE HoursPerWeek >= 38\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM active_employees\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYLUaYjiGi6d",
        "outputId": "fc357867-650f-4296-c664-64e9c3f750b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}