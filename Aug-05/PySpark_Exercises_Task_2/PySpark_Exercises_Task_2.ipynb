{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1: Setup & SparkSession Initialization"
      ],
      "metadata": {
        "id": "cFwth5-NYbkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        "    Install and configure PySpark in your local system or Colab.\n",
        "\n",
        "    Initialize Spark\n",
        "\n",
        "    Create a DataFrame\n",
        "\n",
        "    Show schema, explain data types, and convert to RDD.\n",
        "\n",
        "    Print .collect() and df.rdd.map() output.\n"
      ],
      "metadata": {
        "id": "51Ccr9ynYfBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2tbe0ABgFF-H"
      },
      "outputs": [],
      "source": [
        "# Install and configure PySpark in your local system or Colab.\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark with:\n",
        "spark = SparkSession.builder \\\n",
        "  .appName(\"BotCampus PySpark Practice\") \\\n",
        "  .master(\"local[*]\") \\\n",
        "  .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from:\n",
        "data = [\n",
        "  (\"Anjali\", \"Bangalore\", 24),\n",
        "  (\"Ravi\", \"Hyderabad\", 28),\n",
        "  (\"Kavya\", \"Delhi\", 22),\n",
        "  (\"Meena\", \"Chennai\", 25),\n",
        "  (\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "wESwL9-yYTB3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show schema, explain data types, and convert to RDD.\n",
        "df.printSchema()\n",
        "df.show()\n",
        "\n",
        "# Convert to RDD\n",
        "rdd = df.rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmwd29e5Y0lc",
        "outputId": "be80dbc3-3374-4534-bede-c46d9035cd43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n",
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Anjali|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "| Arjun|   Mumbai| 30|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Print .collect() and df.rdd.map() output.\n",
        "print(rdd.collect())\n",
        "print(rdd.map(lambda x: (x.name.upper(), x.age)).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFTrlwTBY3X9",
        "outputId": "f42de8ce-e0c9-4e68-f36d-98cd65e64d02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n",
            "[('ANJALI', 24), ('RAVI', 28), ('KAVYA', 22), ('MEENA', 25), ('ARJUN', 30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: RDDs & Transformations"
      ],
      "metadata": {
        "id": "SbJC9RyHZge0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario: You received app feedback from users in free-text.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    Split each line into words ( flatMap ).\n",
        "\n",
        "    Remove stop words ( from , the , etc.).\n",
        "\n",
        "    Count each word frequency using reduceByKey .\n",
        "\n",
        "    Find top 3 most frequent non-stop words."
      ],
      "metadata": {
        "id": "lNqe_4QQZjMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario: You received app feedback from users in free-text.\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "  \"Ravi from Bangalore loved the delivery\",\n",
        "  \"Meena from Hyderabad had a late order\",\n",
        "  \"Ajay from Pune liked the service\",\n",
        "  \"Anjali from Delhi faced UI issues\",\n",
        "  \"Rohit from Mumbai gave positive feedback\"\n",
        "])\n",
        "\n",
        "stop_words = {\"from\", \"the\", \"a\", \"an\", \"with\", \"had\"}"
      ],
      "metadata": {
        "id": "xPEifLFUZeSw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = (\n",
        "    feedback\n",
        "\n",
        "    # Split each line into words ( flatMap ).\n",
        "    .flatMap(lambda line: line.lower().split())\n",
        "\n",
        "    # Remove stop words ( from , the , etc.).\n",
        "    .filter(lambda word: word not in stop_words)\n",
        "\n",
        "    # Count each word frequency using reduceByKey .\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")"
      ],
      "metadata": {
        "id": "7BESm9sDZz-m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 3 most frequent non-stop words.\n",
        "top3 = word_counts.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfY7AaPeZ6Q6",
        "outputId": "0609f31c-2aca-42cc-c0c6-265adc0b38e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('loved', 1), ('liked', 1), ('service', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3: DataFrames & Transformation (With Joins)"
      ],
      "metadata": {
        "id": "DxYkImh0bHe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        "    Join both DataFrames on name .\n",
        "\n",
        "    Create a new column: attendance_rate = days_present / 25 .\n",
        "\n",
        "    Grade students using when :A: >90, B: 80–90, C: <80.\n",
        "\n",
        "    Filter students with good grades but poor attendance (<80%)."
      ],
      "metadata": {
        "id": "HPrrGTP0bLCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrames:\n",
        "students = [\n",
        "  (\"Amit\", \"10-A\", 89),\n",
        "  (\"Kavya\", \"10-B\", 92),\n",
        "  (\"Anjali\", \"10-A\", 78),\n",
        "  (\"Rohit\", \"10-B\", 85),\n",
        "  (\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "attendance = [\n",
        "  (\"Amit\", 24),\n",
        "  (\"Kavya\", 22),\n",
        "  (\"Anjali\", 20),\n",
        "  (\"Rohit\", 25),\n",
        "  (\"Sneha\", 19)\n",
        "]\n",
        "columns2 = [\"name\", \"days_present\"]"
      ],
      "metadata": {
        "id": "U_FPLaClaolj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join both DataFrames on name .\n",
        "df_students = spark.createDataFrame(students, columns)\n",
        "df_attendance = spark.createDataFrame(attendance, columns2)"
      ],
      "metadata": {
        "id": "aRAYlpTpbhD6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column: attendance_rate = days_present / 25 .\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "df_joined = df_students.join(df_attendance, \"name\")\n",
        "df_joined = df_joined.withColumn(\"attendance_rate\", col(\"days_present\") / 25)\n",
        "\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBkMag3Pbkat",
        "outputId": "d2ed7c67-7bad-4169-9d38-ff9afe4ec665"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+\n",
            "|  name|section|marks|days_present|attendance_rate|\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  Amit|   10-A|   89|          24|           0.96|\n",
            "|Anjali|   10-A|   78|          20|            0.8|\n",
            "| Kavya|   10-B|   92|          22|           0.88|\n",
            "| Rohit|   10-B|   85|          25|            1.0|\n",
            "| Sneha|   10-C|   80|          19|           0.76|\n",
            "+------+-------+-----+------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grade students using when :A: >90, B: 80–90, C: <80.\n",
        "df_joined = df_joined.withColumn(\"grade\",\n",
        "    when(col(\"marks\") > 90, \"A\")\n",
        "    .when((col(\"marks\") > 80) & (col(\"marks\") <= 90), \"B\")\n",
        "    .when((col(\"marks\") >= 70) & (col(\"marks\") <= 80), \"C\")\n",
        "    .otherwise(\"D\")\n",
        ")\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxCYCFqbboCR",
        "outputId": "a87c6142-ecc8-43c9-8286-13e570b528b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  name|section|marks|days_present|attendance_rate|grade|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  Amit|   10-A|   89|          24|           0.96|    B|\n",
            "|Anjali|   10-A|   78|          20|            0.8|    C|\n",
            "| Kavya|   10-B|   92|          22|           0.88|    A|\n",
            "| Rohit|   10-B|   85|          25|            1.0|    B|\n",
            "| Sneha|   10-C|   80|          19|           0.76|    C|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter students with good grades but poor attendance (<80%).\n",
        "df_filtered = df_joined.filter((col(\"grade\").isin(\"A\", \"B\")) & (col(\"attendance_rate\") < 0.8))\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS4Xdmplbt2_",
        "outputId": "22e7d589-3807-4014-cc88-a8ea5cc0a931"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+-----+------------+---------------+-----+\n",
            "|name|section|marks|days_present|attendance_rate|grade|\n",
            "+----+-------+-----+------------+---------------+-----+\n",
            "+----+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Ingest CSV & JSON, Save to Parquet"
      ],
      "metadata": {
        "id": "NdEsUfPxcs2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        "    1. Ingest CSV:\n",
        "    2. Ingest JSON:\n",
        "\n",
        "    Read both formats into DataFrames.\n",
        "\n",
        "    Flatten nested JSON using select , col , alias , explode .\n",
        "\n",
        "    Save both as Parquet files partitioned by city."
      ],
      "metadata": {
        "id": "_785gU8KcyPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ingest CSV:\n",
        "csv_data = [(\"101\",\"Anil\",\"IT\",\"Bangalore\",80000),\n",
        "            (\"102\",\"Kiran\",\"HR\",\"Mumbai\",65000),\n",
        "            (\"103\",\"Deepa\",\"Finance\",\"Chennai\",72000)]\n",
        "columns = [\"emp_id\", \"name\", \"dept\", \"city\", \"salary\"]\n",
        "df_csv = spark.createDataFrame(csv_data, columns)\n",
        "df_csv.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xIaAHYSclre",
        "outputId": "1a59e546-bc26-49cf-b242-f505a7bd5dde"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+---------+------+\n",
            "|emp_id| name|   dept|     city|salary|\n",
            "+------+-----+-------+---------+------+\n",
            "|   101| Anil|     IT|Bangalore| 80000|\n",
            "|   102|Kiran|     HR|   Mumbai| 65000|\n",
            "|   103|Deepa|Finance|  Chennai| 72000|\n",
            "+------+-----+-------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Ingest JSON:\n",
        "json_data = [{\n",
        "    \"id\": 201,\n",
        "    \"name\": \"Nandini\",\n",
        "    \"contact\": {\n",
        "        \"email\": \"nandi@example.com\",\n",
        "        \"city\": \"Hyderabad\"\n",
        "    },\n",
        "    \"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        "}]"
      ],
      "metadata": {
        "id": "U0BwY2BsdI4x"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read both formats into DataFrames.\n",
        "import pandas as pd\n",
        "import json\n",
        "df_json = spark.read.json(spark.sparkContext.parallelize([json.dumps(json_data[0])]))\n",
        "\n",
        "\n",
        "# Flatten nested JSON using select , col , alias , explode .\n",
        "from pyspark.sql.functions import col, explode\n",
        "\n",
        "flattened = df_json.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    col(\"contact.email\").alias(\"email\"),\n",
        "    col(\"contact.city\").alias(\"city\"),\n",
        "    explode(\"skills\").alias(\"skill\")\n",
        ")\n",
        "flattened.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IMDOGwmdTw7",
        "outputId": "f2b98617-565f-4fde-97e4-199ab1ad808c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------------+---------+------+\n",
            "| id|   name|            email|     city| skill|\n",
            "+---+-------+-----------------+---------+------+\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Python|\n",
            "|201|Nandini|nandi@example.com|Hyderabad| Spark|\n",
            "|201|Nandini|nandi@example.com|Hyderabad|   SQL|\n",
            "+---+-------+-----------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as Parquet\n",
        "df_csv.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/tmp/output/employees_csv\")\n",
        "flattened.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/tmp/output/employees_json\")"
      ],
      "metadata": {
        "id": "hkzIBC5mdVfs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 5: Spark SQL with Temp Views"
      ],
      "metadata": {
        "id": "30DlPv60d-nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        "    Register the students DataFrame as students_view .\n",
        "\n",
        "    Write and run the following queries:\n",
        "    \n",
        "    a) Average marks per section\n",
        "    b) Top scorer in each section\n",
        "    c) Count of students in each grade category\n",
        "    d) Students with marks above class average\n",
        "    e) Attendance-adjusted performance"
      ],
      "metadata": {
        "id": "Llk1opSHd_sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the students DataFrame as students_view .\n",
        "df_joined.createOrReplaceTempView(\"students_view\")"
      ],
      "metadata": {
        "id": "U8SHJwcMeIrx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Average marks per section\n",
        "spark.sql(\"\"\"\n",
        "    SELECT section, ROUND(AVG(marks), 2) AS avg_marks\n",
        "    FROM students_view\n",
        "    GROUP BY section\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwYTmP76fG-4",
        "outputId": "b651fa21-a52b-4260-fd7d-55941dd2f2fb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|section|avg_marks|\n",
            "+-------+---------+\n",
            "|   10-C|     80.0|\n",
            "|   10-A|     83.5|\n",
            "|   10-B|     88.5|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Top scorer in each section\n",
        "spark.sql(\"\"\"\n",
        "    SELECT section, name, marks\n",
        "    FROM (\n",
        "        SELECT *, ROW_NUMBER() OVER (PARTITION BY section ORDER BY marks DESC) as rank\n",
        "        FROM students_view\n",
        "    ) ranked\n",
        "    WHERE rank = 1\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo5frc3LfNc3",
        "outputId": "dbdb8506-91b7-4ced-98d2-4a399157215d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|section| name|marks|\n",
            "+-------+-----+-----+\n",
            "|   10-A| Amit|   89|\n",
            "|   10-B|Kavya|   92|\n",
            "|   10-C|Sneha|   80|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Count of students in each grade category\n",
        "spark.sql(\"\"\"\n",
        "    SELECT grade, COUNT(*) AS student_count\n",
        "    FROM students_view\n",
        "    GROUP BY grade\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec434sUbfQEm",
        "outputId": "9cd7b628-13b9-479c-a9f7-3df528f8f03b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|grade|student_count|\n",
            "+-----+-------------+\n",
            "|    B|            2|\n",
            "|    C|            2|\n",
            "|    A|            1|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Students with marks above class average\n",
        "spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM students_view\n",
        "    WHERE marks > (SELECT AVG(marks) FROM students_view)\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IahhHWSfP8l",
        "outputId": "e93410d0-ef23-49e8-e328-dddbf8b46706"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+------------+---------------+-----+\n",
            "| name|section|marks|days_present|attendance_rate|grade|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "| Amit|   10-A|   89|          24|           0.96|    B|\n",
            "|Kavya|   10-B|   92|          22|           0.88|    A|\n",
            "|Rohit|   10-B|   85|          25|            1.0|    B|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e) Attendance-adjusted performance\n",
        "# (Example: adjusted_score = 0.7 * marks + 0.3 * attendance)\n",
        "spark.sql(\"\"\"\n",
        "    SELECT name, section, marks, days_present,\n",
        "           ROUND(0.7 * marks + 0.3 * days_present, 2) AS adjusted_score\n",
        "    FROM students_view\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS4pR1EOfPyL",
        "outputId": "d44e0d5c-5520-42c4-ed5d-1b024789c216"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+--------------+\n",
            "|  name|section|marks|days_present|adjusted_score|\n",
            "+------+-------+-----+------------+--------------+\n",
            "|  Amit|   10-A|   89|          24|          69.5|\n",
            "|Anjali|   10-A|   78|          20|          60.6|\n",
            "| Kavya|   10-B|   92|          22|          71.0|\n",
            "| Rohit|   10-B|   85|          25|          67.0|\n",
            "| Sneha|   10-C|   80|          19|          61.7|\n",
            "+------+-------+-----+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6: Partitioned Data & Incremental Loading"
      ],
      "metadata": {
        "id": "SvRBP953g6XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Full Load\n",
        "\n",
        "Step 2: Incremental Load\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    List files in output/students/ using Python.\n",
        "\n",
        "    Read only partition 10-A and list students.\n",
        "\n",
        "    Compare before/after counts for section 10-A ."
      ],
      "metadata": {
        "id": "iBjkSbCxg9VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Full Load\n",
        "df_students.write.partitionBy(\"section\").mode(\"overwrite\").parquet(\"output/students/\")\n",
        "df_students.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaoB9vfihIfv",
        "outputId": "a09071be-2b9d-41ff-ddc5-bde7d770ef2f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+\n",
            "|  name|section|marks|\n",
            "+------+-------+-----+\n",
            "|  Amit|   10-A|   89|\n",
            "| Kavya|   10-B|   92|\n",
            "|Anjali|   10-A|   78|\n",
            "| Rohit|   10-B|   85|\n",
            "| Sneha|   10-C|   80|\n",
            "+------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Incremental Load\n",
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"output/students/\")\n",
        "df_inc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftIoSoV4ics8",
        "outputId": "206d482b-d862-4705-fb60-8deaf55ca53b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+\n",
            "| name|section|marks|\n",
            "+-----+-------+-----+\n",
            "|Tejas|   10-A|   91|\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in output/students/ using Python.\n",
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"output/students/\"):\n",
        "    for f in files:\n",
        "        print(os.path.join(root, f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzTVilmxixYd",
        "outputId": "7e63cc35-21ac-4f32-fcee-4904c25ec0ce"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output/students/._SUCCESS.crc\n",
            "output/students/_SUCCESS\n",
            "output/students/section=10-A/.part-00001-8a1466c1-9339-42ed-beb5-c76c661a1032.c000.snappy.parquet.crc\n",
            "output/students/section=10-A/part-00001-8a1466c1-9339-42ed-beb5-c76c661a1032.c000.snappy.parquet\n",
            "output/students/section=10-A/part-00001-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet\n",
            "output/students/section=10-A/.part-00001-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet.crc\n",
            "output/students/section=10-A/part-00000-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet\n",
            "output/students/section=10-A/.part-00000-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet.crc\n",
            "output/students/section=10-B/part-00001-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet\n",
            "output/students/section=10-B/.part-00001-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet.crc\n",
            "output/students/section=10-B/part-00000-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet\n",
            "output/students/section=10-B/.part-00000-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet.crc\n",
            "output/students/section=10-C/part-00001-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet\n",
            "output/students/section=10-C/.part-00001-97d225d0-6390-425e-bfcc-6c2ca04fd5af.c000.snappy.parquet.crc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read only partition 10-A and list students.\n",
        "df_10a = spark.read.parquet(\"output/students/section=10-A\")\n",
        "df_10a.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMECo39di9gt",
        "outputId": "35057920-5e59-4ef6-b44a-87dc26bcd84f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  name|marks|\n",
            "+------+-----+\n",
            "|Anjali|   78|\n",
            "| Tejas|   91|\n",
            "|  Amit|   89|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare before/after counts for section 10-A .\n",
        "\n",
        "# Before adding incremental record\n",
        "df_before = spark.read.parquet(\"output/students/section=10-A\")\n",
        "count_before = df_before.count()\n",
        "\n",
        "# After incremental load\n",
        "df_after = spark.read.parquet(\"output/students/\")\n",
        "count_after = df_after.filter(col(\"section\") == \"10-A\").count()\n",
        "\n",
        "print(f\"Before count: {count_before}, After count: {count_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN8aXV2bjA4O",
        "outputId": "c659ef62-b69a-4c1c-8d14-bd1bee6d22eb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before count: 3, After count: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Module 7: ETL Pipeline – End to End"
      ],
      "metadata": {
        "id": "IEejcNMLjkRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        "    Load CSV with inferred schema.\n",
        "\n",
        "    Fill null bonuses with 2000 .\n",
        "\n",
        "    Create total_ctc = salary + bonus .\n",
        "\n",
        "    Filter employees with total_ctc > 65000 .\n",
        "\n",
        "    Save result in:\n",
        "      JSON format.\n",
        "      Parquet format partitioned by department."
      ],
      "metadata": {
        "id": "TgoXbElTjrHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "4U3O8W0qj0qF",
        "outputId": "ff2025ad-fafb-4424-f04f-5240b5439b80"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c29d7ebd-d81e-4d89-8752-99a584b1c070\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c29d7ebd-d81e-4d89-8752-99a584b1c070\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving employee_raw.csv to employee_raw.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV with inferred schema.\n",
        "df_employees = spark.read.csv(\"employee_raw.csv\", header=True, inferSchema=True)\n",
        "df_employees.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHGUcCI0kwMg",
        "outputId": "0111ba5d-0d72-471b-ceee-5b6be2bfb905"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| NULL|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| NULL|\n",
            "+------+------+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill null bonuses with 2000 .\n",
        "from pyspark.sql.functions import coalesce\n",
        "\n",
        "df_filled = df.fillna({'bonus': 2000})\n",
        "df_filled.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAM3I9_Wk_lB",
        "outputId": "e0c02b5e-2094-4e58-f81e-920f5a7d8a0b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| 2000|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|\n",
            "+------+------+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create total_ctc = salary + bonus .\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_total_ctc = df_filled.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus\"))\n",
        "df_total_ctc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72vomMWAlXlz",
        "outputId": "dbcb09d1-23e2-4786-826c-eab023a8b86c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+---------+\n",
            "|emp_id|  name|   dept|salary|bonus|total_ctc|\n",
            "+------+------+-------+------+-----+---------+\n",
            "|     1| Arjun|     IT| 75000| 5000|    80000|\n",
            "|     2| Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3| Sneha|Finance| 68000| 4000|    72000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|    60000|\n",
            "+------+------+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter employees with total_ctc > 65000.\n",
        "filtered_df = df_total_ctc.filter(col(\"total_ctc\") > 65000)\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNsITcAvleef",
        "outputId": "da87a5b8-6482-4a71-dbc2-790fcd64d1a0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 75000| 5000|    80000|\n",
            "|     3|Sneha|Finance| 68000| 4000|    72000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save result in:\n",
        "#   JSON format.\n",
        "filtered_df.write.mode(\"overwrite\").json(\"/tmp/clean_employees/json\")\n",
        "\n",
        "#   Parquet format partitioned by department.\n",
        "filtered_df.write.mode(\"overwrite\").partitionBy(\"dept\").parquet(\"/tmp/clean_employees/parquet\")\n",
        "# Read Parquet directory into a DataFrame\n",
        "parquet_df = spark.read.parquet(\"/tmp/clean_employees/parquet\")\n",
        "parquet_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKN097fulqpM",
        "outputId": "33890d76-74f0-4a85-9982-3eb9e45f7b1f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+------+-----+---------+-------+\n",
            "|emp_id| name|salary|bonus|total_ctc|   dept|\n",
            "+------+-----+------+-----+---------+-------+\n",
            "|     3|Sneha| 68000| 4000|    72000|Finance|\n",
            "|     1|Arjun| 75000| 5000|    80000|     IT|\n",
            "+------+-----+------+-----+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the JSON file\n",
        "# Convert Spark DataFrame to Pandas\n",
        "pandas_df = filtered_df.toPandas()\n",
        "\n",
        "# Save as a single JSON file\n",
        "json_path = \"/tmp/clean_employees.json\"\n",
        "pandas_df.to_json(json_path, orient=\"records\", lines=True)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(json_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5TGloE9JmUuW",
        "outputId": "cdadda9a-b23a-4755-e61c-b1ebcb626c7a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e7e3c24e-0312-4f12-90d3-41610128fcae\", \"clean_employees.json\", 177)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}